# Tokenization in NLP

## Basic terminologies

1. __Corpus__ - Paragraph is usually called as Corpus.
2. __Documents__ - Sentences
3. __Vocabulary__ - All unique words available in the paragraph(corpus).
4. __Words__ - Words in corpus are called as words itself.

## Tokenization

Conside this paragraph(Corpus): 
> My name is Jeswin and I like to play football. And my favorite football player is Messi. I like coding also.

__Tokenization__ is a process in which we take either a paragraph or sentences and we convert these into smaller parts known as Tokens. When we perform tokenization on the above paragraph, the tokens that are generated are called sentences(or documents). We can also convert the above paragraph into words.

When we do tokenization from a paragraph to sentence we will look into these kind of characters like full stop(.) or exclamation(!). We can do this with python also.

1st sentence/document => My name is Jeswin and I like to play football  
2nd sentence/document => And my favorite football player is Messi  

Now we can do tokenization on a sentence and can convert each word in that sentence into a seperate word. So in short, __words can also be a token and sentences can also be a token__.

Tokenization is a part of text-preprocessing and it is required because each and every word can be converted into vector.

Consider these sentences:
> I like to drink Apple Juice. My friend likes Mango Juice. 

So above there are 2 sentences (since 2 full stops) and we will perform tokenization and the tokens present here will be sentences.

So there will be two tokens:
1. I like to drink Apple Juice
2. My friend likes Mango Juice

So now from these sentences we can count the total number of words i.e., 11. 
But number of unique words are: 10. 

So in the paragraph above the vocbulary (unique words) are the 10 unique words.

[Difference between NLTK and spaCy](https://www.kaggle.com/discussions/general/299223)

![](images\2.PNG)
